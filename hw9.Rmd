---
title: "STA6703 SML HW9"
# author: Youmin Li
#----------------------
## ch9 - q5

set.seed(1)
n <- 500
x1 <- runif(n) - 0.5
x2 <- runif(n) - 0.5
y  <- as.factor(1 * (x1^2 - x2^2 > 0))
dat <- data.frame(x1, x2, y)

# (c) plain logistic (linear boundary)

m_lin <- glm(y ~ x1 + x2, family = binomial, data = dat)

# (e) logistic with non-linear terms

m_nl  <- glm(y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2),
             family = binomial, data = dat)

# (g,h) SVMs

library(e1071)
svm_lin <- svm(y ~ x1 + x2, data = dat, kernel = "linear", cost = 1)
svm_rbf <- svm(y ~ x1 + x2, data = dat, kernel = "radial", gamma = 5, cost = 1)

# quick training errors

mean(predict(m_lin, type="response") > .5 != (y=="1"))
mean(predict(m_nl,  type="response") > .5 != (y=="1"))
mean(predict(svm_lin, dat) != y)
mean(predict(svm_rbf, dat) != y)

## ch9 - q8 -----------

library(ISLR2)
library(e1071)
set.seed(1)

oj <- OJ
id_train <- sample(seq_len(nrow(oj)), 800)
train <- oj[id_train, ]
test  <- oj[-id_train, ]

# Linear SVC (tune cost)

t_lin <- tune(svm, Purchase ~ ., data = train,
              kernel = "linear",
              ranges = list(cost = 10^seq(-3, 3, by = 1)))
svm_lin <- t_lin$best.model
tab_lin <- table(pred = predict(svm_lin, test), truth = test$Purchase)
err_lin <- 1 - sum(diag(tab_lin)) / sum(tab_lin)

# RBF SVM (tune gamma, cost)

t_rbf <- tune(svm, Purchase ~ ., data = train,
              kernel = "radial",
              ranges = list(cost = 10^seq(-2, 3, by = 1),
                            gamma = 10^seq(-3, 1, by = 1)))
svm_rbf <- t_rbf$best.model
tab_rbf <- table(pred = predict(svm_rbf, test), truth = test$Purchase)
err_rbf <- 1 - sum(diag(tab_rbf)) / sum(tab_rbf)

# Polynomial SVM (tune degree, cost)

t_poly <- tune(svm, Purchase ~ ., data = train,
               kernel = "polynomial",
               ranges = list(cost = 10^seq(-2, 3, by = 1),
                             degree = 2:4))

svm_poly <- t_poly$best.model
tab_poly <- table(pred = predict(svm_poly, test), truth = test$Purchase)
err_poly <- 1 - sum(diag(tab_poly)) / sum(tab_poly)

list(test_error = c(linear = err_lin, rbf = err_rbf, poly = err_poly))

# ch12, q3
X <- matrix(c(1,4, 1,3, 0,4, 5,1, 6,2, 4,0), ncol = 2, byrow = TRUE)
colnames(X) <- c("X1","X2")
rownames(X) <- paste0("Obs", 1:6)

# (a) Plot
plot(X, pch = 19, cex = 1.5, xlab = "X1", ylab = "X2")
text(X, labels = rownames(X), pos = 3, cex = 0.8)

# (b)-(e) K-means with K=2 (multiple starts for stability)
set.seed(42)
km <- kmeans(X, centers = 2, nstart = 50)
km$cluster
km$centers

# Final plot with cluster colors
cols <- c("#1b9e77", "#d95f02")[km$cluster]
plot(X, pch = 19, col = cols, cex = 1.6, xlab = "X1", ylab = "X2",
     main = "K=2 clusters (converged)")
text(X, labels = rownames(X), pos = 3, cex = 0.8)
points(km$centers, pch = 8, cex = 2, lwd = 2)
legend("bottomright", legend = paste("Cluster", 1:2),
       col = c("#1b9e77", "#d95f02"), pch = 19, bty = "n")
       
### chapter 12 q9
# (a) Complete-linkage clustering on raw features

d_raw <- dist(USArrests, method = "euclidean")
hc_raw <- hclust(d_raw, method = "complete")
plot(hc_raw, main = "USArrests (raw): complete linkage")

# (b) Cut to K=3 clusters and list memberships
grp_raw <- cutree(hc_raw, k = 3)
split(names(grp_raw), grp_raw)

# (c) Repeat with scaled variables
Z <- scale(USArrests)
d_scl <- dist(Z, method = "euclidean")
hc_scl <- hclust(d_scl, method = "complete")
plot(hc_scl, main = "USArrests (scaled): complete linkage")

grp_scl <- cutree(hc_scl, k = 3)
split(names(grp_scl), grp_scl)

# (d) Simple comparison table: how many states moved clusters?
table(grp_raw, grp_scl)


### chapter12 q10
set.seed(123)

# (a) Simulate: 3 classes, 20 obs each, p=50 features, distinct means
n_per <- 20; p <- 50
mu1 <- rep(0, p)
mu2 <- c(rep(2.5, 5), rep(0, p-5))
mu3 <- c(rep(-2.5, 5), rep(0, p-5))

X1 <- matrix(rnorm(n_per * p, mean = 0, sd = 1), n_per, p) + matrix(mu1, n_per, p, byrow=TRUE)
X2 <- matrix(rnorm(n_per * p, mean = 0, sd = 1), n_per, p) + matrix(mu2, n_per, p, byrow=TRUE)
X3 <- matrix(rnorm(n_per * p, mean = 0, sd = 1), n_per, p) + matrix(mu3, n_per, p, byrow=TRUE)

X <- rbind(X1, X2, X3)
y <- factor(rep(1:3, each = n_per))

# (b) PCA and PC1-PC2 plot
pr <- prcomp(scale(X), center = TRUE, scale. = TRUE)
pc <- pr$x[, 1:2]
plot(pc, col = y, pch = 19, xlab = "PC1", ylab = "PC2",
     main = "PC1 vs PC2 (colored by true class)")
legend("topright", legend = paste("Class", 1:3), col = 1:3, pch = 19, bty = "n")

# (c) K-means K=3 on all features
km3_all <- kmeans(scale(X), centers = 3, nstart = 50)
table(Truth = y, KM = km3_all$cluster)

# (d) K=2, (e) K=4
km2_all <- kmeans(scale(X), centers = 2, nstart = 50)
km4_all <- kmeans(scale(X), centers = 4, nstart = 50)
table(y, km2_all$cluster); table(y, km4_all$cluster)

# (f) K=3 on first two PCs only
km3_pc2 <- kmeans(pc, centers = 3, nstart = 50)
table(Truth = y, KMpc2 = km3_pc2$cluster)

# (g) (Optional) Purity helper to quantify recovery
purity <- function(true, pred) {
  tab <- table(true, pred)
  sum(apply(tab, 2, max)) / length(true)
}
c(purity_3_all = purity(y, km3_all$cluster),
  purity_3_pc2 = purity(y, km3_pc2$cluster))

